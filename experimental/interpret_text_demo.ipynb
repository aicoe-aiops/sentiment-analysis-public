{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Interpreting Bert </h2>\n",
    "\n",
    "The aim of this notebook is to try understand how BERT models make decisions. To do this we take the airline tweets dataset and perform sentiment analysis on it using BERT. We then make use of interpret-text , an opensource library to help us understand our trained model.\n",
    "\n",
    "This process would take a lot of time in a CPU environment hence GPU is strongly recommended.\n",
    "To have the dashboard load up , please make sure you have jswidgets enabled in your jupyter environment.\n",
    "<br>\n",
    "\n",
    "Use the following jupyterhub image for this notebook:\n",
    "<br>\n",
    "tensorflow-gpu-3.6-CUDA10.1\n",
    "\n",
    "\n",
    "Run the following command and restart kernel to make sure it is enabled : \n",
    "<br>\n",
    "<b>jupyter nbextension enable --py --sys-prefix widgetsnbextension</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scrapbook as sb\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from interpret_text.experimental.common.utils_bert import Language, Tokenizer, BERTSequenceClassifier\n",
    "from interpret_text.experimental.common.timer import Timer\n",
    "import re\n",
    "from sklearn.linear_model.base import MultiOutputMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from interpret_text.experimental.unified_information import UnifiedInformationExplainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done importing the dependencies , we import our data into the dataframe , perform some pre processing and split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Tweets.csv')\n",
    "\n",
    "#Shuffling the data\n",
    "tweets.sample(frac=1)\n",
    "\n",
    "features = tweets.iloc[:, 10].values\n",
    "labels = tweets.iloc[:, 1].values\n",
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    #Getting rid of special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    processed_features.append(processed_feature)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_train = label_encoder.fit_transform(y_train)\n",
    "labels_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some hyper parameters for our mode before we set it up for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_FRACTION = 1\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "torch.cuda.set_device(0) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 1\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "DATA_FOLDER = \"./temp\"\n",
    "BERT_CACHE_DIR = \"./temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 50\n",
    "BATCH_SIZE_PRED = 512\n",
    "TRAIN_SIZE = 0.6\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize our text using a tokenizer provided by the interpret-text library itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(Language.ENGLISH, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)\n",
    "tokens_train = tokenizer.tokenize(X_train)\n",
    "tokens_test = tokenizer.tokenize(X_test)\n",
    "\n",
    "tokens_train, mask_train, _ = tokenizer.preprocess_classification_tokens(tokens_train, MAX_LEN)\n",
    "tokens_test, mask_test, _ = tokenizer.preprocess_classification_tokens(tokens_test, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTSequenceClassifier(language=LANGUAGE, num_labels=3, cache_dir=BERT_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the model, keeping track of the time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Timer() as t:\n",
    "#     classifier.fit(token_ids=tokens_train,\n",
    "#                     input_mask=mask_train,\n",
    "#                     labels=labels_train,    \n",
    "#                     num_epochs=NUM_EPOCHS,\n",
    "#                     batch_size=BATCH_SIZE,    \n",
    "#                     verbose=True)    \n",
    "# print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed the training. In the next steps we perform predictions on test data and train our explainer.\n",
    "\n",
    "If you have already run the model before just uncomment and load the pretrained model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(classifier,'saved_model.pth')\n",
    "classifier = torch.load('saved_model.pth')\n",
    "preds = classifier.predict(token_ids=tokens_test, \n",
    "                           input_mask=mask_test, \n",
    "                           batch_size=BATCH_SIZE_PRED)\n",
    "\n",
    "report = classification_report(labels_test, preds, target_names=label_encoder.classes_, output_dict=True) \n",
    "accuracy = accuracy_score(labels_test, preds)\n",
    "print(\"accuracy: {}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have decided to use Unified Information Explainer for this model we need to initialize and train with our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(report, indent=4, sort_keys=True))\n",
    "\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
    "\n",
    "classifier.model.to(device)\n",
    "for param in classifier.model.parameters():\n",
    "    param.requires_grad = False\n",
    "classifier.model.eval()\n",
    "\n",
    "#Target Layer is the parameter which decides   \n",
    "interpreter_unified = UnifiedInformationExplainer(model=classifier.model, \n",
    "                                 train_dataset=list(X_train), \n",
    "                                 device=device, \n",
    "                                 target_layer=14, \n",
    "                                 classes=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use an of the test samples, make a predictions and use the dashboard for the said prediction.\n",
    "We can use indeces to select test samples or we can just use a sentences and label of our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1010\n",
    "text = X_test[idx]\n",
    "true_label = y_test[idx]\n",
    "predicted_label = label_encoder.inverse_transform([preds[idx]])\n",
    "print(text, true_label, predicted_label)\n",
    "\n",
    "explanation_unified = interpreter_unified.explain_local(text, true_label)\n",
    "from interpret_text.experimental.widget import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(explanation_unified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}