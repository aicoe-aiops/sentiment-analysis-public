{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Sentiment Analysis of Tweets using BERT </h1>\n",
    "\n",
    "In this notebook we will go through the process of classiying tweets(or any text data for that matter) into positive,negative or neutral.\n",
    "The dataset we use for this task is the [Airline Tweets Dataset](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)\n",
    "\n",
    "We will be using [MLFlow](https://mlflow.org/) to track our traininig process.\n",
    "\n",
    "If you are not running it via a jupyterhub image but locally or by cloning the repository,to set up the environment please refer to this [doc](https://docs.google.com/document/d/1BUEzAeymOr1NyWQT4_vY22dFlMinjcbeV6iFBZhBTYY/edit) and the requirements.txt in the repository\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First ,we will download the pre trained model and files required which allow us to use it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip -P models/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import fileinput\n",
    "import string\n",
    "import zipfile\n",
    "import datetime\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "import mlflow\n",
    "from pandas import DataFrame\n",
    "sys.path.insert(0, 'models/bert')\n",
    "from models.bert import modeling\n",
    "from models.bert import optimization\n",
    "from models.bert import run_classifier\n",
    "from models.bert import tokenization\n",
    "\n",
    "#extracting the downloaded model\n",
    "folder = 'models/bert'\n",
    "with zipfile.ZipFile(\"models/bert/uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initalize the MLFlow client in the following step so that we can track our run and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_CLIENT = mlflow.tracking.MlflowClient(tracking_uri='http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com')\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server-route-aiops-prod-prometheus-scrape.cloud.paas.psi.redhat.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('sentiment_analysis_test_0.1')\n",
    "mlflow.start_run(run_name=\"airline_tweets-trialrun-same-artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id = mlflow.active_run().info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64f2c3a0c296d5bb724b405d18a8df438db444da"
   },
   "source": [
    "# BERT implementation\n",
    "\n",
    "We are going to use Google's pre trained BERT for our classification tasks. \n",
    "Apart from the model itself we also directly use Google's scripts to run our classifier which enables us to use the model for our data specifically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading and Cleaning data</h2>\n",
    "\n",
    "First we load up our data in the csv format into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1aa95f622d1b59645a9e22c2e6d5d2a5f29a2a50"
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('dataset/Tweets.csv')\n",
    "\n",
    "#Shuffling the data\n",
    "tweets.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing tweets</h2>\n",
    "\n",
    "We perfrom some basic cleaning on our text data using regular expressions.\n",
    "We then split our data into test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = tweets.iloc[:, 10].values\n",
    "labels = tweets.iloc[:, 1].values\n",
    "#preprocessing \n",
    "processed_features = []\n",
    "\n",
    "for sentence in range(0, len(features)):\n",
    "    #Getting rid of special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "    processed_features.append(processed_feature)\n",
    "\n",
    "#Splitting the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping the emotions to numbers for the training and inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"positive\":2,\"negative\":0,\"neutral\":1}\n",
    "y_train = [d[x] for x in y_train]\n",
    "y_test = [d[x] for x in y_test]\n",
    "\n",
    "print(X_test[10],y_test[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20541f718c86fed1131409bcc9ee7736d3ec88ac"
   },
   "outputs": [],
   "source": [
    "folder = 'models/bert'\n",
    "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
    "BERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\n",
    "OUTPUT_DIR = f'{folder}/outputs'\n",
    "print(f'>> Model output directory: {OUTPUT_DIR}')\n",
    "print(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of the model name as a mlflow run tag\n",
    "mlflow.set_tag(\"model\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training the model</h2>\n",
    "\n",
    "Now that we have our data ready for use we move on the next step i.e training the model on our data.Since we alrady have the pre-learned weights on the model we can get good results by training the model on our data for just a few epochs.\n",
    "\n",
    "We first start by intializing our model and transforming our data ready for consumption by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a128c3f38ab69351e34f87977cec04d9d6c9189"
   },
   "outputs": [],
   "source": [
    "def create_examples(lines, set_type, labels=None):\n",
    "#Generate data for the BERT model. We nned data in this format before being fed for training\n",
    "    guid = f'{set_type}'\n",
    "    examples = []\n",
    "    if guid == 'train':\n",
    "        for line, label in zip(lines, labels):\n",
    "            text_a = line\n",
    "            label = str(label)\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    else:\n",
    "        for line in lines:\n",
    "            text_a = line\n",
    "            label = '0'\n",
    "            examples.append(\n",
    "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "    return examples\n",
    "\n",
    "# Model Hyper Parameters\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "WARMUP_PROPORTION = 0.1\n",
    "#We need this to be a little lower thant the max length of tweets we have \n",
    "MAX_SEQ_LENGTH = 50\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
    "# each checpoint weights about 1,5gb\n",
    "ITERATIONS_PER_LOOP = 100000\n",
    "NUM_TPU_CORES = 8\n",
    "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
    "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
    "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
    "\n",
    "label_list = [str(num) for num in range(3)]\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
    "train_examples = create_examples(X_train, 'train', labels=y_train)\n",
    "\n",
    "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
    "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
    "        num_shards=NUM_TPU_CORES,\n",
    "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "model_fn = run_classifier.model_fn_builder(\n",
    "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
    "    num_labels=len(label_list),\n",
    "    init_checkpoint=INIT_CHECKPOINT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
    "    use_one_hot_embeddings=True)\n",
    "\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    eval_batch_size=EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging parameters into MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log parameters before run\n",
    "mlflow.log_param(\"TRAIN_BATCH_SIZE\", TRAIN_BATCH_SIZE)\n",
    "mlflow.log_param(\"EVAL_BATCH_SIZE\", EVAL_BATCH_SIZE)\n",
    "mlflow.log_param(\"LEARNING_RATE\", LEARNING_RATE)\n",
    "mlflow.log_param(\"NUM_TRAIN_EPOCHS\", NUM_TRAIN_EPOCHS)\n",
    "mlflow.log_param(\"WARMUP_PROPORTION\", WARMUP_PROPORTION)\n",
    "mlflow.log_param(\"MAX_SEQ_LENGTH\", MAX_SEQ_LENGTH)\n",
    "mlflow.log_param(\"SAVE_CHECKPOINTS_STEPS\", SAVE_CHECKPOINTS_STEPS)\n",
    "mlflow.log_param(\"ITERATIONS_PER_LOOP\", ITERATIONS_PER_LOOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "\n",
    "We now train our model accroding to the previously designed hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3cb95e4f486c7743add1039f996f501d07109fe"
   },
   "outputs": [],
   "source": [
    "print('Please wait...')\n",
    "train_features = run_classifier.convert_examples_to_features(\n",
    "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "print('>> Started training at {} '.format(datetime.datetime.now()))\n",
    "print('  Num examples = {}'.format(len(train_examples)))\n",
    "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print('>> Finished training at {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started training at 2020-03-02 21:59:02.759879\n",
    " \n",
    "\n",
    "Finished training at 2020-03-02 23:11:47.571450       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and save model Variables and Protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model_save_path = 'models/saved_models'\n",
    "\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    input_ids = tf.placeholder(dtype=tf.int64, shape=[None, MAX_SEQ_LENGTH], name='input_ids')\n",
    "    input_mask = tf.placeholder(dtype=tf.int64, shape=[None, MAX_SEQ_LENGTH], name='input_mask')\n",
    "    segment_ids = tf.placeholder(dtype=tf.int64, shape=[None, MAX_SEQ_LENGTH], name='segment_ids')\n",
    "    label_ids = tf.placeholder(dtype=tf.int64, shape=[None, ], name='unique_ids')\n",
    "\n",
    "    receive_tensors = {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids,\n",
    "                       'label_ids': label_ids}\n",
    "    features = {'input_ids': input_ids, 'input_mask': input_mask, 'segment_ids': segment_ids, \"label_ids\": label_ids}\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receive_tensors)\n",
    "\n",
    "estimator._export_to_tpu = False\n",
    "estimator.export_saved_model(serving_model_save_path, serving_input_receiver_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predicting and Evaluating</h2>\n",
    "\n",
    "Now that our training step is complete in the next steps we will use what our model learned to make predictions on the dataset. We will then evaluate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ed42f193d4586b10b58a65ea06c3d51385160bb"
   },
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_id)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    print(params)\n",
    "    batch_size = 500\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddf519b690697635cbd3b47d2ee875feb94fd822"
   },
   "outputs": [],
   "source": [
    "predict_examples = create_examples(X_test, 'test')\n",
    "\n",
    "predict_features = run_classifier.convert_examples_to_features(\n",
    "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "predict_input_fn = input_fn_builder(\n",
    "    features=predict_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb9635b3771ecc148b7dc7a2bf4f5de6a8173445"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for prediction in result:\n",
    "      preds.append(np.argmax(prediction['probabilities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5cc9b9374f73ce94a04b9448e6807976cd89ec0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de003a2788e27d334232bd515572d73304192fe5"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of BERT is:\",accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of BERT is: 0.7990654205607477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 Score of BERT is:\",f1_score(y_test, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score of BERT is: 0.660558251784892\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = classification_report(y_test, preds, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputframe = DataFrame(dict(sentence = pd.Series(X_train), old_model_label = pd.Series(y_train), pred_label = pd.Series(preds))).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving our output into a csv for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputframe.to_csv('output/airline_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_Precision \", metrics['macro avg']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_recall \", metrics['macro avg']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_f1-score \",  metrics['macro avg']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Avg_support \",  metrics['macro avg']['support']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"Accuracy \",  accuracy_score(y_test, preds)) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_Precision \",  metrics['0']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_recall \",  metrics['0']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_f1-score \",  metrics['0']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"0_support \",  metrics['0']['support']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_Precision \",  metrics['1']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_recall \",  metrics['1']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_f1-score \",  metrics['1']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"1_support \",  metrics['1']['support']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_Precision \",  metrics['2']['precision'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_recall \",  metrics['2']['recall']) \n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_f1-score \",  metrics['2']['f1-score'])\n",
    "MLFLOW_CLIENT.log_metric(mlflow_run_id, \"2_support \",  metrics['2']['support']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
